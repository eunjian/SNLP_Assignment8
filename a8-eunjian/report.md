-   Technical information 
    - I used Python 3.8 and Jupyter notebook for the implementation of this code.
    -   A list of libraries: 
	    - Data reading: I used pandas library for reading csv files. 
	    - Preprocessing: I used string, nltk(stopwords, PortStemmer) library for data preprocessing. They were used for removing punctuation, stopwords and stemming.
	    - Feature selection: I used sklearn library for feature selections (CountVectorizer, TfidfVectorizer, TfidfTransformer). Two vectorizing ways were used in this assignment. One is countvectorizing and the other is TF-IDF. For TF-IDF, I use TfidfVectorizer for Logistic Regression model and Multinomial NB model and TfidfTransformer with CountVectorizer for SVM model.
	    - Build, train and test the models: I use scikit-learn library (Pipeline, LogisticRegression, SGDClassifier, MultinomialNB) for making and implementing the models. I use Pipeline for building a model with vectorizer at one time. And three kinds of models were used, i.e., LogisticRegression, SGDClassifier, MultinomialNB. 
	    - Evaluation: To check the performance of the models, I used scikit-learn library (confusion_matrix). By using confusion matrix, I printed the accuracy, precision, recall and F1 Score of each model.
	    - Parameter tuning: I used scikit-learn library(VotingClassifier, GridSearchCV) for parameter tuning. 
	    - For training and testing the external data: I used scikit-learn library(train_test_split) for the external data. I trained and tested the models by using an external data from dataworld website regarding gender classifier.
    -   Links to additional data or resources used:
	    - Error metric implementation: https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea
	    - Grid search for tuning the Linear Regression and MultinomialNB models: https://www.markhneedham.com/blog/2017/12/10/scikit-learn-using-gridsearch-tune-hyper-parameters-votingclassifier/
	    - Grid search for SVM: https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html
	    - To download the external data: https://data.world/crowdflower/gender-classifier-data
	    - For external data preprocessing: https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b
-   A description of approach:
    -   Used models: Among many machine learning models, I chose to use three different models for this assignment. Linear Regression, Naive Bayes and Support Vector Machine (SVM) models were used. The reason is that those were quite general and primitive models in natural language processing area. So, it was easy to find some information and tutorials for implementing and also we covered those models in the class. I wanted to find out which model could give the best performance in this gender classification assignment. 
 There was one weird thing in my implementation. Before training the models with external data, Linear regression and Naive Bayes models with TF-IDF vectorizer tend to give the biased prediction of female. So, the precision and F1 score were either very low, or even sometimes, all of the results were consisted with only female and it was not able to calculate them. However, after I trained the models with external data, it gives better results. Therefore, I guess the problem before using the external data was because of too small amount of training data.
    -   How to tune the models: I found out several parameter tuning online tutorials and followed their implementation. For Linear Regression and Naive Bayes models, I wanted to find out which model is better to use. Also to see which gives better results whether using unigram or bigram and using TF-IDF or not. 
    -   Preprocessing or feature selection steps:
	    - Preprocessing: For data preprocessing, I removed the punctuation marks in the text by import string library, specifically string.punctuation. Then, I discarded the stopwords in the text. All of the text data were in English, so I used collected stopwords list from nltk.corpus. Lastly, I did the stemming with the use of PorterStemmer from nltk library. I also found out many tutorials with lemmatization, but in my case stemming worked better than lemmatizing. So I decided to use stemming only.
	    - Feature selection: For the feature selection, I considered countvectorizer, TF-IDF and n-grams. As seen in the parameter tuning part, the model with TF-IDF and without countvectorizer nor multi n-grams was considered as the best model in Naive Bayes and Linear Regression models. In SVM models, TF-IDF and count vectorizer did not have significant different performance, but the model with TF-IDF slightly showed better performance than the one with count vectorizer.
    -   External data: I got an external data from the website called data.world for additional training of the models. The aim of this data set was to train a CrowdFlower AI gender predictor. I preprocessed the original data and also changee the format of it. The data was collected from Twitter, so I removed the web address, emojis and etc. I made the data frame to CSV file which contains the label and text information. 
-   A report of the results:
    -   Scores for the models: The final best result was shown from the SVM model. The performance was better when I split the external data into training data and test data for training and testing the model. In that case, the SVM model with TF-IDF vectorizer showed the best performance with an accuracy of around 0.65. However, when I use the external data for training and test the model with the provided test data set, the overall performance of all the models got worse. This time, the SVM model with a count vectorizer had a better result than with TF-IDF. Another interesting point was that the parameter tuning works well and had significant improvement in this case. When I trained and tested the model with the external data only, even the best models got minor improvement like the accuracy from 0.651 to 0.654. On the contrary, the best model which used the original test data set had an accuracy of 0.48 before parameter tuning. And it showed the result after parameter tuning with an accuracy of 0.56. 