{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intimate-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data reading\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('essays-train.txt', sep='\\t', header=0, encoding='utf-8')\n",
    "test = pd.read_csv('essays-test.txt', sep='\\t', header=0, encoding='utf-8')\n",
    "ext = pd.read_csv('ext_data.csv', sep='\\t', header=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "divided-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data preprocessing\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess(data):\n",
    "    # removing punctuation\n",
    "    data['text'] = data['text'].apply(lambda x: x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))))\n",
    "\n",
    "    # removing stopwords\n",
    "    sw = stopwords.words('english')\n",
    "    data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "\n",
    "    # stemming\n",
    "    st = PorterStemmer()\n",
    "    data['text'] = data['text'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# preprocess the text data of training data, test data and external data\n",
    "train = preprocess(train)\n",
    "test = preprocess(test)\n",
    "external_data = preprocess(ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "experienced-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Features selection and Modelling to build a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# vectorizing - Countvectorizing, TF-IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
    "# different models - Logistic Regression, Support Vector Machine and Multinomial Naive Bayes.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn import svm\n",
    "\n",
    "\n",
    "def pipeline(model,feat):\n",
    "    if model == 'LR':\n",
    "        # Logistic regression model using countvectors and TF-IDF\n",
    "        if feat == 'count':\n",
    "            return Pipeline([('vect', CountVectorizer()), ('clf', LogisticRegression())])\n",
    "        if feat == 'tfidf':\n",
    "            return Pipeline([('tfidf', TfidfVectorizer(min_df=6, max_features=None, strip_accents='unicode',\n",
    "                                                       analyzer=\"word\", token_pattern=r'\\w{1,}', ngram_range=(1, 2),\n",
    "                                                       use_idf=1, smooth_idf=1, sublinear_tf=1)), \n",
    "                             ('clf', LogisticRegression())])\n",
    "    \n",
    "    if model == 'NB':\n",
    "        # Multinomial Naive Bayes model using countvectors and TF-IDF\n",
    "        if feat == 'count':\n",
    "            return Pipeline([('vect', CountVectorizer()), ('clf', MultinomialNB())])\n",
    "        if feat == 'tfidf':\n",
    "            return Pipeline([('tfidf', TfidfVectorizer(min_df=6, max_features=None, strip_accents='unicode',\n",
    "                                                       analyzer=\"word\", token_pattern=r'\\w{1,}', ngram_range=(1, 2),\n",
    "                                                       use_idf=1, smooth_idf=1, sublinear_tf=1)),\n",
    "                             ('clf', MultinomialNB())])    \n",
    "    if model == 'SVM':\n",
    "        # SVM model using countvectors and TF-IDF\n",
    "        if feat == 'count':\n",
    "            return Pipeline([('vect', CountVectorizer()), ('clf', SGDClassifier())])\n",
    "        if feat == 'tfidf':\n",
    "            return Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SGDClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "familiar-colombia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy: 0.44\n",
      "Precision: 0.375\n",
      "Recall: 0.6\n",
      "F1 Score: 0.462\n",
      "\n",
      "Accuracy: 0.6\n",
      "Precision: 0.5\n",
      "Recall: 0.1\n",
      "F1 Score: 0.167\n",
      "\n",
      "Naive Bayes\n",
      "Accuracy: 0.56\n",
      "Precision: 0.4\n",
      "Recall: 0.2\n",
      "F1 Score: 0.267\n",
      "\n",
      "Accuracy: 0.56\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: nan\n",
      "\n",
      "SVM\n",
      "Accuracy: 0.48\n",
      "Precision: 0.412\n",
      "Recall: 0.7\n",
      "F1 Score: 0.519\n",
      "\n",
      "Accuracy: 0.48\n",
      "Precision: 0.412\n",
      "Recall: 0.7\n",
      "F1 Score: 0.519\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-bd065f865d2e>:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1 = round(2 * (precision * recall) / (precision + recall),3)\n"
     ]
    }
   ],
   "source": [
    "### train the data and make a prediction of the test data\n",
    "\n",
    "def fit_predict(model, feat, x, y, test_x):\n",
    "    clf = pipeline(model, feat).fit(x, y)\n",
    "    pred = clf.predict(test_x)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "### See the performance of each model's error metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def error_metric(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    true_neg, false_pos = cm[0]\n",
    "    false_neg, true_pos = cm[1]\n",
    "    accuracy = round((true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg),3)\n",
    "    precision = round((true_pos) / (true_pos + false_pos),3)\n",
    "    recall = round((true_pos) / (true_pos + false_neg),3)\n",
    "    f1 = round(2 * (precision * recall) / (precision + recall),3)\n",
    "    \n",
    "    print('Accuracy: {}'.format(accuracy))\n",
    "    print('Precision: {}'.format(precision))\n",
    "    print('Recall: {}'.format(recall))\n",
    "    print('F1 Score: {}'.format(f1))\n",
    "    print()\n",
    "\n",
    "# Logistic Regression classifier\n",
    "predicted_LR_count = fit_predict('LR','count',train['text'],train['label'],test['text'])\n",
    "predicted_LR_tfidf = fit_predict('LR','tfidf',train['text'],train['label'],test['text'])\n",
    "print('Logistic Regression')\n",
    "error_metric(test['label'],predicted_LR_count)\n",
    "error_metric(test['label'],predicted_LR_tfidf)\n",
    "\n",
    "# NB classifier\n",
    "predicted_NB_count = fit_predict('NB','count',train['text'],train['label'],test['text'])\n",
    "predicted_NB_tfidf = fit_predict('NB','tfidf',train['text'],train['label'],test['text'])\n",
    "print('Naive Bayes')\n",
    "error_metric(test['label'],predicted_NB_count)\n",
    "error_metric(test['label'],predicted_NB_tfidf)\n",
    "\n",
    "# SVM classifier\n",
    "predicted_svm_count = fit_predict('SVM','count',train['text'],train['label'],test['text'])\n",
    "predicted_svm_tfidf = fit_predict('SVM','tfidf',train['text'],train['label'],test['text'])\n",
    "print('SVM')\n",
    "error_metric(test['label'],predicted_svm_count)\n",
    "error_metric(test['label'],predicted_svm_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "pending-globe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  35 | elapsed:   15.4s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:   15.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'voting__weights': [0, 0, 1]} -0.6903748056575155\n",
      "{'voting__weights': [0, 1, 0]} -0.9001929366617831\n",
      "{'voting__weights': [0, 1, 1]} -0.7255989832187952\n",
      "{'voting__weights': [1, 0, 0]} -5.161628084769083\n",
      "{'voting__weights': [1, 0, 1]} -0.8728606507322535\n",
      "{'voting__weights': [1, 1, 0]} -1.0171236426355847\n",
      "{'voting__weights': [1, 1, 1]} -0.8080595851313813\n",
      "Best score: -0.690\n",
      "Best parameters set:\n",
      "\tvoting__weights: [0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "### Parameter tuning by using gridsearch\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# check logistic regression vs. naive bayes model\n",
    "# check countvector vs. tf-idf\n",
    "\n",
    "unigram_log_pipe = Pipeline([('cv', CountVectorizer()),('logreg', LogisticRegression())])\n",
    "\n",
    "ngram_pipe = Pipeline([('cv', CountVectorizer(ngram_range=(1, 2))), ('mnb', MultinomialNB())])\n",
    "\n",
    "tfidf_pipe = Pipeline([('tfidf', TfidfVectorizer()), ('mnb', MultinomialNB())])\n",
    "\n",
    "classifiers = [(\"ngram\", ngram_pipe), (\"unigram\", unigram_log_pipe), (\"tfidf\", tfidf_pipe),]\n",
    "\n",
    "mixed_pipe = Pipeline([(\"voting\", VotingClassifier(classifiers, voting=\"soft\"))])\n",
    "\n",
    "def combinations_on_off(num_classifiers):\n",
    "    return [[int(x) for x in list(\"{0:0b}\".format(i).zfill(num_classifiers))]\n",
    "            for i in range(1, 2 ** num_classifiers)]\n",
    "\n",
    "param_grid = dict(voting__weights=combinations_on_off(len(classifiers)))\n",
    "grid_search = GridSearchCV(mixed_pipe, param_grid=param_grid, n_jobs=-1, verbose=10, scoring=\"neg_log_loss\")\n",
    "\n",
    "grid_search.fit(train['text'],train['label'])\n",
    "\n",
    "cv_results = grid_search.cv_results_\n",
    "\n",
    "for mean_score, params in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n",
    "    print(params, mean_score)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "# represents [ngram_pipe, unigram_log_pipe classifiers, tfidf_pipe]\n",
    "# for example, [0, 0, 1] means we should use only tf-idf and not use bigrams nor logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cardiac-malaysia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.584\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-05\n",
      "\tclf__max_iter: 20\n",
      "\tclf__penalty: 'l2'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__ngram_range: (1, 1)\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best score: 0.607\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-05\n",
      "\tclf__max_iter: 20\n",
      "\tclf__penalty: 'l2'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__ngram_range: (1, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    2.9s finished\n"
     ]
    }
   ],
   "source": [
    "# grid search for SVM model\n",
    "\n",
    "param_svm = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'clf__max_iter': (20,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "}\n",
    "\n",
    "# find the best parameters for both the feature extraction and the classifier\n",
    "def grid_search(pipeline, parameters, X_train, y_train):\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "    best_model = grid_search.fit(X_train, y_train)\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return best_model\n",
    "        \n",
    "# SVM model grid search\n",
    "best_svm_count = grid_search(pipeline('SVM','count'),param_svm,train['text'],train['label'])\n",
    "best_svm_tfidf = grid_search(pipeline('SVM','tfidf'),param_svm,train['text'],train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "appointed-wiring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   32.4s finished\n",
      "C:\\Users\\aej06\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.648\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__max_iter: 20\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\tvect__max_df: 0.5\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   23.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.649\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-05\n",
      "\tclf__max_iter: 20\n",
      "\tclf__penalty: 'l2'\n",
      "\tvect__max_df: 0.75\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   30.4s finished\n",
      "C:\\Users\\aej06\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   30.5s finished\n",
      "C:\\Users\\aej06\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB model\n",
      "Accuracy: 0.644\n",
      "Precision: 0.66\n",
      "Recall: 0.56\n",
      "F1 Score: 0.606\n",
      "\n",
      "SVM models\n",
      "Accuracy: 0.643\n",
      "Precision: 0.658\n",
      "Recall: 0.561\n",
      "F1 Score: 0.606\n",
      "\n",
      "Accuracy: 0.651\n",
      "Precision: 0.656\n",
      "Recall: 0.603\n",
      "F1 Score: 0.628\n",
      "\n",
      "SVM models after parameter tuning\n",
      "Accuracy: 0.65\n",
      "Precision: 0.648\n",
      "Recall: 0.622\n",
      "F1 Score: 0.635\n",
      "\n",
      "Accuracy: 0.656\n",
      "Precision: 0.643\n",
      "Recall: 0.667\n",
      "F1 Score: 0.655\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### use external data\n",
    "# I will not use logistic regression model and countvectors\n",
    "# since we see the result of grid search.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(external_data['text'], external_data['label'], test_size = 0.33, random_state = 0)\n",
    "\n",
    "# NB classifier\n",
    "predicted_NB_tfidf = fit_predict('NB','tfidf', X_train, y_train, X_test)\n",
    "\n",
    "# SVM classifier\n",
    "predicted_svm_count = fit_predict('SVM','count', X_train, y_train, X_test)\n",
    "predicted_svm_tfidf = fit_predict('SVM','tfidf', X_train, y_train, X_test)\n",
    "\n",
    "# additionally, grid search for the best SVM model\n",
    "best_svm_count = grid_search(pipeline('SVM','count'),param_svm,X_train, y_train)\n",
    "best_svm_tfidf = grid_search(pipeline('SVM','tfidf'),param_svm,X_train, y_train)\n",
    "# and test those best models\n",
    "best_svm_count.fit(X_train, y_train)\n",
    "pred_best_count = best_svm_count.predict(X_test)\n",
    "best_svm_tfidf.fit(X_train, y_train)\n",
    "pred_best_tfidf = best_svm_tfidf.predict(X_test)\n",
    "\n",
    "\n",
    "# show error metric of each model (using only external data for training and testing the model)\n",
    "print('MultinomialNB model')\n",
    "error_metric(y_test,predicted_NB_tfidf)\n",
    "\n",
    "print('SVM models')\n",
    "error_metric(y_test,predicted_svm_count)\n",
    "error_metric(y_test,predicted_svm_tfidf)\n",
    "\n",
    "print('SVM models after parameter tuning')\n",
    "error_metric(y_test,pred_best_count)\n",
    "error_metric(y_test,pred_best_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "danish-stockholm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   34.7s finished\n",
      "C:\\Users\\aej06\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.652\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__max_iter: 20\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\tvect__max_df: 0.75\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   41.9s finished\n",
      "C:\\Users\\aej06\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.659\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-05\n",
      "\tclf__max_iter: 20\n",
      "\tclf__penalty: 'elasticnet'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__ngram_range: (1, 2)\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   26.6s finished\n",
      "C:\\Users\\aej06\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   26.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB model\n",
      "Accuracy: 0.48\n",
      "Precision: 0.429\n",
      "Recall: 0.9\n",
      "F1 Score: 0.581\n",
      "\n",
      "SVM models\n",
      "Accuracy: 0.48\n",
      "Precision: 0.435\n",
      "Recall: 1.0\n",
      "F1 Score: 0.606\n",
      "\n",
      "Accuracy: 0.44\n",
      "Precision: 0.417\n",
      "Recall: 1.0\n",
      "F1 Score: 0.589\n",
      "\n",
      "SVM models after parameter tuning\n",
      "Accuracy: 0.56\n",
      "Precision: 0.476\n",
      "Recall: 1.0\n",
      "F1 Score: 0.645\n",
      "\n",
      "Accuracy: 0.48\n",
      "Precision: 0.435\n",
      "Recall: 1.0\n",
      "F1 Score: 0.606\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aej06\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:570: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    }
   ],
   "source": [
    "# train the model with external data \n",
    "# and use provided test data for testing the model\n",
    "\n",
    "# NB classifier\n",
    "predicted_NB_tfidf = fit_predict('NB','tfidf', external_data['text'], external_data['label'], test['text'])\n",
    "\n",
    "# SVM classifier\n",
    "predicted_svm_count = fit_predict('SVM','count',external_data['text'], external_data['label'], test['text'])\n",
    "predicted_svm_tfidf = fit_predict('SVM','tfidf',external_data['text'], external_data['label'], test['text'])\n",
    "\n",
    "# additionally, grid search for the best SVM model\n",
    "best_svm_count = grid_search(pipeline('SVM','count'),param_svm,external_data['text'], external_data['label'])\n",
    "best_svm_tfidf = grid_search(pipeline('SVM','tfidf'),param_svm,external_data['text'], external_data['label'])\n",
    "# and test those best models\n",
    "best_svm_count.fit(X_train, y_train)\n",
    "pred_best_count = best_svm_count.predict(test['text'])\n",
    "best_svm_tfidf.fit(X_train, y_train)\n",
    "pred_best_tfidf = best_svm_tfidf.predict(test['text'])\n",
    "\n",
    "# show error metric of each model \n",
    "print('MultinomialNB model')\n",
    "error_metric(test['label'],predicted_NB_tfidf)\n",
    "\n",
    "print('SVM models')\n",
    "error_metric(test['label'],predicted_svm_count)\n",
    "error_metric(test['label'],predicted_svm_tfidf)\n",
    "\n",
    "print('SVM models after parameter tuning')\n",
    "error_metric(test['label'],pred_best_count)\n",
    "error_metric(test['label'],pred_best_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-cover",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
